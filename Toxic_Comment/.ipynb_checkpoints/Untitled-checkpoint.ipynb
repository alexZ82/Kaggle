{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 id                                       comment_text  toxic  \\\n",
      "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
      "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
      "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
      "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
      "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
      "\n",
      "   severe_toxic  obscene  threat  insult  identity_hate  \n",
      "0             0        0       0       0              0  \n",
      "1             0        0       0       0              0  \n",
      "2             0        0       0       0              0  \n",
      "3             0        0       0       0              0  \n",
      "4             0        0       0       0              0  \n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('train.csv')\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 id                                       comment_text  toxic  \\\n",
      "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
      "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
      "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
      "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
      "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
      "\n",
      "   severe_toxic  obscene  threat  insult  identity_hate  neutral  \n",
      "0             0        0       0       0              0        1  \n",
      "1             0        0       0       0              0        1  \n",
      "2             0        0       0       0              0        1  \n",
      "3             0        0       0       0              0        1  \n",
      "4             0        0       0       0              0        1  \n"
     ]
    }
   ],
   "source": [
    "train_data['neutral'] = train_data.apply(lambda x: 0 if sum(x[2:8])>=1 else 1 ,axis = 1)\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 0 1]\n",
      " [0 0 0 ..., 0 0 1]\n",
      " [0 0 0 ..., 0 0 1]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 1]\n",
      " [0 0 0 ..., 0 0 1]\n",
      " [0 0 0 ..., 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "labels = np.array(train_data[['toxic',\"severe_toxic\",  \"obscene\" , \"threat\" , \"insult\",  \"identity_hate\",  \"neutral\"]])\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 id                                       comment_text  toxic  \\\n",
      "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
      "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
      "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
      "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
      "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
      "\n",
      "   severe_toxic  obscene  threat  insult  identity_hate  neutral  \\\n",
      "0             0        0       0       0              0        1   \n",
      "1             0        0       0       0              0        1   \n",
      "2             0        0       0       0              0        1   \n",
      "3             0        0       0       0              0        1   \n",
      "4             0        0       0       0              0        1   \n",
      "\n",
      "                                           processed  \n",
      "0  explanation why the edits made under my userna...  \n",
      "1  d aww  he matches this background colour i m s...  \n",
      "2  hey man  i m really not trying to edit war  it...  \n",
      "3    more i can t make any real suggestions on im...  \n",
      "4  you  sir  are my hero  any chance you remember...  \n"
     ]
    }
   ],
   "source": [
    "def process_comment_text(txt):\n",
    "    ntxt = re.sub(r\"[^a-zA-Z]\", \" \", txt)\n",
    "    ntxt = ntxt.lower()\n",
    "    return ntxt\n",
    "\n",
    "train_data['processed'] = train_data.comment_text.apply(process_comment_text)\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_words = 10000\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(train_data.processed)\n",
    "sequences = tokenizer.texts_to_sequences(train_data.processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 168816 unique tokens.\n",
      "[1 1 1 ..., 1 1 1]\n",
      "<class 'numpy.ndarray'>\n",
      "Shape of data tensor: (159571, 500)\n",
      "Shape of label tensor: (159571,)\n"
     ]
    }
   ],
   "source": [
    "maxlen = 500\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "labels = np.asarray(train_data.neutral)\n",
    "print(labels)\n",
    "print(type(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#x_train,x_test,y_train,y_test = train_test_split(data,labels,test_size = 0.2,stratify=labels)\n",
    "x_train,x_test,y_train,y_test = train_test_split(data,labels,test_size = 0.2)\n",
    "x_train,x_val,y_train,y_val = train_test_split(x_train,y_train,test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#x_train_n = x_train[y_train==1]\n",
    "#y_train_n = np.ones(x_train_n.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10337\n",
      "91787\n"
     ]
    }
   ],
   "source": [
    "print(len(y_train[y_train==0]))\n",
    "print(len(y_train[y_train==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102124, 500)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# glove word embedding\n",
    "glove_dir = '/Users/sapaz3/Documents/TextMining/book_mat/glove.6B'\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 500, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 50000)             0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 32)                1600032   \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,600,065\n",
      "Trainable params: 2,600,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "  \n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.93973106  0.55630972]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "class_weight = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "print(class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 102124 samples, validate on 25532 samples\n",
      "Epoch 1/1\n",
      "102124/102124 [==============================] - 108s - loss: 0.0862 - acc: 0.9728 - val_loss: 0.2450 - val_acc: 0.9435\n"
     ]
    }
   ],
   "source": [
    "#class_weight = {0 : 9.,1: 1.}\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=1,\n",
    "                    batch_size=32,\n",
    "                    validation_data = (x_val,y_val), class_weight = class_weight)#validation_split = 0.2)\n",
    "model.save_weights('pre_trained_glove_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.00000000e+00],\n",
       "       [  9.98600423e-01],\n",
       "       [  8.80551577e-01],\n",
       "       ..., \n",
       "       [  9.99979615e-01],\n",
       "       [  3.03436651e-11],\n",
       "       [  9.91717577e-01]], dtype=float32)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31776/31915 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.22971673327187064, 0.94485351715494281]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9448535171549428"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = np.array([1 if y> 0.5 else 0 for y in y_prob])\n",
    "len(y_test[classes == y_test])/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2108  1120]\n",
      " [  640 28047]]\n",
      "0.969578594393\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "print(confusion_matrix(y_test,classes))\n",
    "print(f1_score(y_test,classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5787476280834914"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1220/2108"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2108,  1120,   640, 28047])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,classes).ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2748"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2108+640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3228"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test[y_test==0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSFLHDAFJ6pghIEkdMwQkqWOGgCR17P8DpSNbQ5Xr+BcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x165727550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "normal_error_df = error_df[(error_df['true_class']== 1) & (error_df['reconstruction_error'] < 4.278143e+04)]\n",
    "_ = ax.hist(normal_error_df.reconstruction_error.values, bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dim = x_train_n.shape[1]\n",
    "encoding_dim = 14\n",
    "\n",
    "input_layer = Input(shape=(input_dim, ))\n",
    "\n",
    "encoder = Dense(encoding_dim, activation=\"tanh\", \n",
    "                activity_regularizer=regularizers.l1(10e-5))(input_layer)\n",
    "encoder = Dense(int(encoding_dim / 2), activation=\"relu\")(encoder)\n",
    "\n",
    "decoder = Dense(int(encoding_dim / 2), activation='tanh')(encoder)\n",
    "decoder = Dense(input_dim, activation='relu')(decoder)\n",
    "\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 91796 samples, validate on 25532 samples\n",
      "Epoch 1/50\n",
      "91796/91796 [==============================] - 6s - loss: 367439.5998 - acc: 0.0862 - val_loss: 357735.5270 - val_acc: 0.0902\n",
      "Epoch 2/50\n",
      "91796/91796 [==============================] - 6s - loss: 365440.8749 - acc: 0.0862 - val_loss: 355900.6389 - val_acc: 0.0902\n",
      "Epoch 3/50\n",
      "91796/91796 [==============================] - 6s - loss: 363636.6284 - acc: 0.0862 - val_loss: 354230.6283 - val_acc: 0.0902\n",
      "Epoch 4/50\n",
      "91796/91796 [==============================] - 6s - loss: 361983.5893 - acc: 0.0862 - val_loss: 352704.5567 - val_acc: 0.0902\n",
      "Epoch 5/50\n",
      "91796/91796 [==============================] - 6s - loss: 360471.6139 - acc: 0.0862 - val_loss: 351300.3308 - val_acc: 0.0902\n",
      "Epoch 6/50\n",
      "91796/91796 [==============================] - 6s - loss: 359044.5666 - acc: 0.0862 - val_loss: 349933.9223 - val_acc: 0.0902\n",
      "Epoch 7/50\n",
      "91796/91796 [==============================] - 7s - loss: 357686.2821 - acc: 0.0862 - val_loss: 348670.1111 - val_acc: 0.0902\n",
      "Epoch 8/50\n",
      "91796/91796 [==============================] - 6s - loss: 356431.8349 - acc: 0.0862 - val_loss: 347486.7621 - val_acc: 0.0902\n",
      "Epoch 9/50\n",
      "91796/91796 [==============================] - 6s - loss: 355239.8909 - acc: 0.0862 - val_loss: 346360.9588 - val_acc: 0.0902\n",
      "Epoch 10/50\n",
      "91796/91796 [==============================] - 6s - loss: 354132.4242 - acc: 0.0862 - val_loss: 345321.5772 - val_acc: 0.0902\n",
      "Epoch 11/50\n",
      "91796/91796 [==============================] - 6s - loss: 353092.5321 - acc: 0.0862 - val_loss: 344340.6400 - val_acc: 0.0902\n",
      "Epoch 12/50\n",
      "91796/91796 [==============================] - 6s - loss: 352082.3686 - acc: 0.0607 - val_loss: 343370.3981 - val_acc: 0.0668\n",
      "Epoch 13/50\n",
      "91796/91796 [==============================] - 6s - loss: 351128.1437 - acc: 0.0561 - val_loss: 342481.2864 - val_acc: 0.0466\n",
      "Epoch 14/50\n",
      "91796/91796 [==============================] - 6s - loss: 350249.4372 - acc: 0.0488 - val_loss: 341651.6662 - val_acc: 0.0673\n",
      "Epoch 15/50\n",
      "91796/91796 [==============================] - 6s - loss: 349382.1614 - acc: 0.0605 - val_loss: 340813.6639 - val_acc: 0.0577\n",
      "Epoch 16/50\n",
      "91796/91796 [==============================] - 6s - loss: 348566.9027 - acc: 0.0576 - val_loss: 340047.9015 - val_acc: 0.0593\n",
      "Epoch 17/50\n",
      "91796/91796 [==============================] - 6s - loss: 347780.5232 - acc: 0.0605 - val_loss: 339282.8823 - val_acc: 0.0685\n",
      "Epoch 18/50\n",
      "91796/91796 [==============================] - 6s - loss: 347015.1155 - acc: 0.0561 - val_loss: 338573.2768 - val_acc: 0.0902\n",
      "Epoch 19/50\n",
      "91796/91796 [==============================] - 6s - loss: 346285.2624 - acc: 0.0862 - val_loss: 337835.5484 - val_acc: 0.0902\n",
      "Epoch 20/50\n",
      "91796/91796 [==============================] - 6s - loss: 345560.1911 - acc: 0.0862 - val_loss: 337166.3184 - val_acc: 0.0902\n",
      "Epoch 21/50\n",
      "91796/91796 [==============================] - 6s - loss: 344885.1021 - acc: 0.0862 - val_loss: 336494.6987 - val_acc: 0.0902\n",
      "Epoch 22/50\n",
      "91796/91796 [==============================] - 6s - loss: 344237.7854 - acc: 0.0862 - val_loss: 335896.2241 - val_acc: 0.0902\n",
      "Epoch 23/50\n",
      "91796/91796 [==============================] - 6s - loss: 343622.8097 - acc: 0.0862 - val_loss: 335290.0862 - val_acc: 0.0902\n",
      "Epoch 24/50\n",
      "91796/91796 [==============================] - 6s - loss: 343018.5634 - acc: 0.0862 - val_loss: 334712.5861 - val_acc: 0.0902\n",
      "Epoch 25/50\n",
      "91796/91796 [==============================] - 6s - loss: 342465.8480 - acc: 0.0862 - val_loss: 334182.9481 - val_acc: 0.0902\n",
      "Epoch 26/50\n",
      "91796/91796 [==============================] - 6s - loss: 341911.1890 - acc: 0.0862 - val_loss: 333689.5604 - val_acc: 0.0902\n",
      "Epoch 27/50\n",
      "91796/91796 [==============================] - 6s - loss: 341369.8149 - acc: 0.0862 - val_loss: 333091.5994 - val_acc: 0.0902\n",
      "Epoch 28/50\n",
      "91796/91796 [==============================] - 6s - loss: 340863.2659 - acc: 0.0862 - val_loss: 332632.4078 - val_acc: 0.0902\n",
      "Epoch 29/50\n",
      "91796/91796 [==============================] - 6s - loss: 340400.1098 - acc: 0.0862 - val_loss: 332135.0739 - val_acc: 0.0902\n",
      "Epoch 30/50\n",
      "91796/91796 [==============================] - 6s - loss: 339904.3386 - acc: 0.0862 - val_loss: 331672.2072 - val_acc: 0.0902\n",
      "Epoch 31/50\n",
      "47040/91796 [==============>...............] - ETA: 3s - loss: 340934.8755 - acc: 0.0874"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-0679b56e64ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                     callbacks=[checkpointer, tensorboard]).history\n\u001b[0m",
      "\u001b[0;32m/Users/sapaz3/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1596\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1598\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m     def evaluate(self, x, y,\n",
      "\u001b[0;32m/Users/sapaz3/anaconda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sapaz3/anaconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sapaz3/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sapaz3/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sapaz3/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sapaz3/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sapaz3/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nb_epoch = 50\n",
    "batch_size = 32\n",
    "\n",
    "autoencoder.compile(optimizer='adam', \n",
    "                    loss='mean_squared_error', \n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=\"model.h5\",\n",
    "                               verbose=0,\n",
    "                               save_best_only=True)\n",
    "tensorboard = TensorBoard(log_dir='./logs',\n",
    "                          histogram_freq=0,\n",
    "                          write_graph=True,\n",
    "                          write_images=True)\n",
    "\n",
    "history = autoencoder.fit(x_train_n, x_train_n,\n",
    "                    epochs=nb_epoch,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(x_val, x_val),\n",
    "                    verbose=1,\n",
    "                    callbacks=[checkpointer, tensorboard]).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "categories = ['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']\n",
    "\n",
    "\"\"\" Retrieve train newsgroup dataset \"\"\"\n",
    "twenty_train = fetch_20newsgroups(subset='train',\n",
    "                                  # remove=('headers', 'footers', 'quotes'),\n",
    "                                  categories=categories,\n",
    "                                  shuffle=True,\n",
    "                                  random_state=random_state)\n",
    "\n",
    "print(\"Following newsgroups will be tested: {}\".format(twenty_train.target_names))\n",
    "print(\"Number of sentences: {}\".format(len(twenty_train.data)))\n",
    "\n",
    "\"\"\" Retrieve test newsgroup dataset \"\"\"\n",
    "twenty_test = fetch_20newsgroups(subset='test',\n",
    "                                 # remove=('headers', 'footers', 'quotes'),\n",
    "                                 categories=categories,\n",
    "                                 shuffle=True,\n",
    "                                 random_state=random_state)\n",
    "docs_test = twenty_test.data\n",
    "\n",
    "\"\"\" Naive Bayes classifier \"\"\"\n",
    "bayes_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', MultinomialNB())\n",
    "                      ])\n",
    "bayes_clf.fit(twenty_train.data, twenty_train.target)\n",
    "joblib.dump(bayes_clf, \"bayes_20newsgroup.pkl\", compress=9)\n",
    "\"\"\" Predict the test dataset using Naive Bayes\"\"\"\n",
    "predicted = bayes_clf.predict(docs_test)\n",
    "print('Naive Bayes correct prediction: {:4.2f}'.format(np.mean(predicted == twenty_test.target)))\n",
    "print(metrics.classification_report(twenty_test.target, predicted, target_names=twenty_test.target_names))\n",
    "\n",
    "\"\"\" Support Vector Machine (SVM) classifier\"\"\"\n",
    "svm_clf = Pipeline([('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=   5, random_state=42)),\n",
    "])\n",
    "svm_clf.fit(twenty_train.data, twenty_train.target)\n",
    "joblib.dump(svm_clf, \"svm_20newsgroup.pkl\", compress=9)\n",
    "\"\"\" Predict the test dataset using Naive Bayes\"\"\"\n",
    "predicted = svm_clf.predict(docs_test)\n",
    "print('SVM correct prediction: {:4.2f}'.format(np.mean(predicted == twenty_test.target)))\n",
    "print(metrics.classification_report(twenty_test.target, predicted, target_names=twenty_test.target_names))\n",
    "\n",
    "print(metrics.confusion_matrix(twenty_test.target, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "print(\"Following newsgroups will be tested: {}\".format(twenty_train.target_names))\n",
    "print(\"Number of sentences: {}\".format(len(twenty_train.data)))\n",
    "\n",
    "\"\"\" Retrieve test newsgroup dataset \"\"\"\n",
    "twenty_test = fetch_20newsgroups(subset='test',\n",
    "                                 # remove=('headers', 'footers', 'quotes'),\n",
    "                                 categories=categories,\n",
    "                                 shuffle=True,\n",
    "                                 random_state=random_state)\n",
    "docs_test = twenty_test.data\n",
    "\n",
    "\"\"\" Naive Bayes classifier \"\"\"\n",
    "bayes_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', MultinomialNB())\n",
    "                      ])\n",
    "bayes_clf.fit(twenty_train.data, twenty_train.target)\n",
    "joblib.dump(bayes_clf, \"bayes_20newsgroup.pkl\", compress=9)\n",
    "\"\"\" Predict the test dataset using Naive Bayes\"\"\"\n",
    "predicted = bayes_clf.predict(docs_test)\n",
    "print('Naive Bayes correct prediction: {:4.2f}'.format(np.mean(predicted == twenty_test.target)))\n",
    "print(metrics.classification_report(twenty_test.target, predicted, target_names=twenty_test.target_names))\n",
    "\n",
    "\"\"\" Support Vector Machine (SVM) classifier\"\"\"\n",
    "svm_clf = Pipeline([('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=   5, random_state=42)),\n",
    "])\n",
    "svm_clf.fit(twenty_train.data, twenty_train.target)\n",
    "joblib.dump(svm_clf, \"svm_20newsgroup.pkl\", compress=9)\n",
    "\"\"\" Predict the test dataset using Naive Bayes\"\"\"\n",
    "predicted = svm_clf.predict(docs_test)\n",
    "print('SVM correct prediction: {:4.2f}'.format(np.mean(predicted == twenty_test.target)))\n",
    "print(metrics.classification_report(twenty_test.target, predicted, target_names=twenty_test.target_names))\n",
    "\n",
    "print(metrics.confusion_matrix(twenty_test.target, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "\n",
    "categories = train_data.columns[2:8]\n",
    "print(categories)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "bayes_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', MultinomialNB())\n",
    "                      ])\n",
    "\n",
    "bayes_clf.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Retrieve train newsgroup dataset \"\"\"\n",
    "categories = ['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']\n",
    "\n",
    "twenty_train = fetch_20newsgroups(subset='train',\n",
    "                                  # remove=('headers', 'footers', 'quotes'),\n",
    "                                  categories=categories,\n",
    "                                  shuffle=True,\n",
    "                                  random_state=random_state)\n",
    "\n",
    "print(twenty_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(twenty_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_train.data[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>\n",
    "#         Olivier Grisel <olivier.grisel@ensta.org>\n",
    "#         Mathieu Blondel <mathieu@mblondel.org>\n",
    "#         Lars Buitinck\n",
    "# License: BSD 3 clause\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import logging\n",
    "import numpy as np\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "# Display progress logs on stdout\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s %(message)s')\n",
    "\n",
    "\n",
    "# parse commandline arguments\n",
    "op = OptionParser()\n",
    "op.add_option(\"--report\",\n",
    "              action=\"store_true\", dest=\"print_report\",\n",
    "              help=\"Print a detailed classification report.\")\n",
    "op.add_option(\"--chi2_select\",\n",
    "              action=\"store\", type=\"int\", dest=\"select_chi2\",\n",
    "              help=\"Select some number of features using a chi-squared test\")\n",
    "op.add_option(\"--confusion_matrix\",\n",
    "              action=\"store_true\", dest=\"print_cm\",\n",
    "              help=\"Print the confusion matrix.\")\n",
    "op.add_option(\"--top10\",\n",
    "              action=\"store_true\", dest=\"print_top10\",\n",
    "              help=\"Print ten most discriminative terms per class\"\n",
    "                   \" for every classifier.\")\n",
    "op.add_option(\"--all_categories\",\n",
    "              action=\"store_true\", dest=\"all_categories\",\n",
    "              help=\"Whether to use all categories or not.\")\n",
    "op.add_option(\"--use_hashing\",\n",
    "              action=\"store_true\",\n",
    "              help=\"Use a hashing vectorizer.\")\n",
    "op.add_option(\"--n_features\",\n",
    "              action=\"store\", type=int, default=2 ** 16,\n",
    "              help=\"n_features when using the hashing vectorizer.\")\n",
    "op.add_option(\"--filtered\",\n",
    "              action=\"store_true\",\n",
    "              help=\"Remove newsgroup information that is easily overfit: \"\n",
    "                   \"headers, signatures, and quoting.\")\n",
    "\n",
    "\n",
    "def is_interactive():\n",
    "    return not hasattr(sys.modules['__main__'], '__file__')\n",
    "\n",
    "# work-around for Jupyter notebook and IPython console\n",
    "argv = [] if is_interactive() else sys.argv[1:]\n",
    "(opts, args) = op.parse_args(argv)\n",
    "if len(args) > 0:\n",
    "    op.error(\"this script takes no arguments.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(__doc__)\n",
    "op.print_help()\n",
    "print()\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Load some categories from the training set\n",
    "if opts.all_categories:\n",
    "    categories = None\n",
    "else:\n",
    "    categories = [\n",
    "        'alt.atheism',\n",
    "        'talk.religion.misc',\n",
    "        'comp.graphics',\n",
    "        'sci.space',\n",
    "    ]\n",
    "\n",
    "if opts.filtered:\n",
    "    remove = ('headers', 'footers', 'quotes')\n",
    "else:\n",
    "    remove = ()\n",
    "\n",
    "print(\"Loading 20 newsgroups dataset for categories:\")\n",
    "print(categories if categories else \"all\")\n",
    "\n",
    "data_train = fetch_20newsgroups(subset='train', categories=categories,\n",
    "                                shuffle=True, random_state=42,\n",
    "                                remove=remove)\n",
    "\n",
    "data_test = fetch_20newsgroups(subset='test', categories=categories,\n",
    "                               shuffle=True, random_state=42,\n",
    "                               remove=remove)\n",
    "print('data loaded')\n",
    "\n",
    "# order of labels in `target_names` can be different from `categories`\n",
    "target_names = data_train.target_names\n",
    "\n",
    "\n",
    "def size_mb(docs):\n",
    "    return sum(len(s.encode('utf-8')) for s in docs) / 1e6\n",
    "\n",
    "data_train_size_mb = size_mb(data_train.data)\n",
    "data_test_size_mb = size_mb(data_test.data)\n",
    "\n",
    "print(\"%d documents - %0.3fMB (training set)\" % (\n",
    "    len(data_train.data), data_train_size_mb))\n",
    "print(\"%d documents - %0.3fMB (test set)\" % (\n",
    "    len(data_test.data), data_test_size_mb))\n",
    "print(\"%d categories\" % len(categories))\n",
    "print()\n",
    "\n",
    "# split a training set and a test set\n",
    "y_train, y_test = data_train.target, data_test.target\n",
    "\n",
    "print(\"Extracting features from the training data using a sparse vectorizer\")\n",
    "t0 = time()\n",
    "if opts.use_hashing:\n",
    "    vectorizer = HashingVectorizer(stop_words='english', alternate_sign=False,\n",
    "                                   n_features=opts.n_features)\n",
    "    X_train = vectorizer.transform(data_train.data)\n",
    "else:\n",
    "    vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                                 stop_words='english')\n",
    "    X_train = vectorizer.fit_transform(data_train.data)\n",
    "duration = time() - t0\n",
    "print(\"done in %fs at %0.3fMB/s\" % (duration, data_train_size_mb / duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % X_train.shape)\n",
    "print()\n",
    "\n",
    "print(\"Extracting features from the test data using the same vectorizer\")\n",
    "t0 = time()\n",
    "X_test = vectorizer.transform(data_test.data)\n",
    "duration = time() - t0\n",
    "print(\"done in %fs at %0.3fMB/s\" % (duration, data_test_size_mb / duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % X_test.shape)\n",
    "print()\n",
    "\n",
    "# mapping from integer feature name to original token string\n",
    "if opts.use_hashing:\n",
    "    feature_names = None\n",
    "else:\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "if opts.select_chi2:\n",
    "    print(\"Extracting %d best features by a chi-squared test\" %\n",
    "          opts.select_chi2)\n",
    "    t0 = time()\n",
    "    ch2 = SelectKBest(chi2, k=opts.select_chi2)\n",
    "    X_train = ch2.fit_transform(X_train, y_train)\n",
    "    X_test = ch2.transform(X_test)\n",
    "    if feature_names:\n",
    "        # keep selected feature names\n",
    "        feature_names = [feature_names[i] for i\n",
    "                         in ch2.get_support(indices=True)]\n",
    "    print(\"done in %fs\" % (time() - t0))\n",
    "    print()\n",
    "\n",
    "if feature_names:\n",
    "    feature_names = np.asarray(feature_names)\n",
    "\n",
    "\n",
    "def trim(s):\n",
    "    \"\"\"Trim string to fit on terminal (assuming 80-column display)\"\"\"\n",
    "    return s if len(s) <= 80 else s[:77] + \"...\"\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Benchmark classifiers\n",
    "def benchmark(clf):\n",
    "    print('_' * 80)\n",
    "    print(\"Training: \")\n",
    "    print(clf)\n",
    "    t0 = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_time = time() - t0\n",
    "    print(\"train time: %0.3fs\" % train_time)\n",
    "\n",
    "    t0 = time()\n",
    "    pred = clf.predict(X_test)\n",
    "    test_time = time() - t0\n",
    "    print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    print(\"accuracy:   %0.3f\" % score)\n",
    "\n",
    "    if hasattr(clf, 'coef_'):\n",
    "        print(\"dimensionality: %d\" % clf.coef_.shape[1])\n",
    "        print(\"density: %f\" % density(clf.coef_))\n",
    "\n",
    "        if opts.print_top10 and feature_names is not None:\n",
    "            print(\"top 10 keywords per class:\")\n",
    "            for i, label in enumerate(target_names):\n",
    "                top10 = np.argsort(clf.coef_[i])[-10:]\n",
    "                print(trim(\"%s: %s\" % (label, \" \".join(feature_names[top10]))))\n",
    "        print()\n",
    "\n",
    "    if opts.print_report:\n",
    "        print(\"classification report:\")\n",
    "        print(metrics.classification_report(y_test, pred,\n",
    "                                            target_names=target_names))\n",
    "\n",
    "    if opts.print_cm:\n",
    "        print(\"confusion matrix:\")\n",
    "        print(metrics.confusion_matrix(y_test, pred))\n",
    "\n",
    "    print()\n",
    "    clf_descr = str(clf).split('(')[0]\n",
    "    return clf_descr, score, train_time, test_time\n",
    "\n",
    "\n",
    "results = []\n",
    "for clf, name in (\n",
    "        (RidgeClassifier(tol=1e-2, solver=\"lsqr\"), \"Ridge Classifier\"),\n",
    "        (Perceptron(n_iter=50), \"Perceptron\"),\n",
    "        (PassiveAggressiveClassifier(n_iter=50), \"Passive-Aggressive\"),\n",
    "        (KNeighborsClassifier(n_neighbors=10), \"kNN\"),\n",
    "        (RandomForestClassifier(n_estimators=100), \"Random forest\")):\n",
    "    print('=' * 80)\n",
    "    print(name)\n",
    "    results.append(benchmark(clf))\n",
    "\n",
    "for penalty in [\"l2\", \"l1\"]:\n",
    "    print('=' * 80)\n",
    "    print(\"%s penalty\" % penalty.upper())\n",
    "    # Train Liblinear model\n",
    "    results.append(benchmark(LinearSVC(penalty=penalty, dual=False,\n",
    "                                       tol=1e-3)))\n",
    "\n",
    "    # Train SGD model\n",
    "    results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50,\n",
    "                                           penalty=penalty)))\n",
    "\n",
    "# Train SGD with Elastic Net penalty\n",
    "print('=' * 80)\n",
    "print(\"Elastic-Net penalty\")\n",
    "results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50,\n",
    "                                       penalty=\"elasticnet\")))\n",
    "\n",
    "# Train NearestCentroid without threshold\n",
    "print('=' * 80)\n",
    "print(\"NearestCentroid (aka Rocchio classifier)\")\n",
    "results.append(benchmark(NearestCentroid()))\n",
    "\n",
    "# Train sparse Naive Bayes classifiers\n",
    "print('=' * 80)\n",
    "print(\"Naive Bayes\")\n",
    "results.append(benchmark(MultinomialNB(alpha=.01)))\n",
    "results.append(benchmark(BernoulliNB(alpha=.01)))\n",
    "\n",
    "print('=' * 80)\n",
    "print(\"LinearSVC with L1-based feature selection\")\n",
    "# The smaller C, the stronger the regularization.\n",
    "# The more regularization, the more sparsity.\n",
    "results.append(benchmark(Pipeline([\n",
    "  ('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\", dual=False,\n",
    "                                                  tol=1e-3))),\n",
    "  ('classification', LinearSVC(penalty=\"l2\"))])))\n",
    "\n",
    "# make some plots\n",
    "\n",
    "indices = np.arange(len(results))\n",
    "\n",
    "results = [[x[i] for x in results] for i in range(4)]\n",
    "\n",
    "clf_names, score, training_time, test_time = results\n",
    "training_time = np.array(training_time) / np.max(training_time)\n",
    "test_time = np.array(test_time) / np.max(test_time)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title(\"Score\")\n",
    "plt.barh(indices, score, .2, label=\"score\", color='navy')\n",
    "plt.barh(indices + .3, training_time, .2, label=\"training time\",\n",
    "         color='c')\n",
    "plt.barh(indices + .6, test_time, .2, label=\"test time\", color='darkorange')\n",
    "plt.yticks(())\n",
    "plt.legend(loc='best')\n",
    "plt.subplots_adjust(left=.25)\n",
    "plt.subplots_adjust(top=.95)\n",
    "plt.subplots_adjust(bottom=.05)\n",
    "\n",
    "for i, c in zip(indices, clf_names):\n",
    "    plt.text(-.3, i, c)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
